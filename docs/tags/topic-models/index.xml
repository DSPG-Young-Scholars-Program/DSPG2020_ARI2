<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>topic models on UVA Biocomplexity Institute</title>
    <link>/tags/topic-models/</link>
    <description>Recent content in topic models on UVA Biocomplexity Institute</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="/tags/topic-models/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Job Performance Scales: LDA Topic Modeling</title>
      <link>/findings/performance_lda/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/findings/performance_lda/</guid>
      <description>/* this chunk of code centers all of the headings */ h1, h2, h3 { text-align: center; }  Approach Once we gathered a sufficient number of scales from a variety of sources, we analyzed these scales using topic modeling to identify underlying themes from across the corpus. Latent Dirichlet Allocation (LDA) is a probabilistic topic modeling process which considers each document as a distribution of topics and each topic as a distribution of words.</description>
    </item>
    
    <item>
      <title>Biterm Modeling of Performance Scales</title>
      <link>/findings/biterm_markdown/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/findings/biterm_markdown/</guid>
      <description>/* this chunk of code centers all of the headings */ h1, h2, h3 { text-align: center; }  About BTM Biterm Topic Modeling (BTM) is a method of detecting the topics occurring in short texts. In other approaches to topic modeling, each document is analyzed for topic occurrences within that document. The challenge with applying traditional topic modeling techniques to short documents is that there is not enough data to work with.</description>
    </item>
    
  </channel>
</rss>