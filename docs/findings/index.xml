<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Findings on UVA Biocomplexity Institute</title>
    <link>/DSPG2020_ARI2/findings/</link>
    <description>Recent content in Findings on UVA Biocomplexity Institute</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="/DSPG2020_ARI2/findings/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Exploring Job Performance Scales</title>
      <link>/DSPG2020_ARI2/findings/performance_tidy_text/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/DSPG2020_ARI2/findings/performance_tidy_text/</guid>
      <description>/* this chunk of code centers all of the headings */ h1, h2, h3 { text-align: center; }  Approach Once we compiled a collection of papers addressing job performance, we extracted scales from those that contained measurement items. The following table displays the scales we collected. Before we began topic modeling, we wanted to get a sense of the sources and content of the scale items.</description>
    </item>
    
    <item>
      <title>Latent Dirichlet Allocation Modeling of Performance Scales</title>
      <link>/DSPG2020_ARI2/findings/performance_lda/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/DSPG2020_ARI2/findings/performance_lda/</guid>
      <description>/* this chunk of code centers all of the headings */ h1, h2, h3 { text-align: center; }  Approach Once we gathered a sufficient number of scales from a variety of sources, we analyzed these scales using topic modeling to identify underlying themes from across the corpus. Latent Dirichlet Allocation (LDA) is a probabilistic topic modeling process which considers each document as a distribution of topics and each topic as a distribution of words.</description>
    </item>
    
    <item>
      <title>Biterm Modeling of Performance Scales</title>
      <link>/DSPG2020_ARI2/findings/biterm_markdown/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/DSPG2020_ARI2/findings/biterm_markdown/</guid>
      <description>/* this chunk of code centers all of the headings */ h1, h2, h3 { text-align: center; }  About BTM Biterm Topic Modeling (BTM) is a method of detecting the topics occurring in short texts. In other approaches to topic modeling, each document is analyzed for topic occurrences within that document. The challenge with applying traditional topic modeling techniques to short documents is that there is not enough data to work with.</description>
    </item>
    
  </channel>
</rss>