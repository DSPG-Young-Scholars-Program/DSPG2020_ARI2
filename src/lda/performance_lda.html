---
title: "Job Performance Scales: LDA Topic Modeling"
description: "This page demonstrates an LDA Topic Modeling approach to our corpus."
tags: ["R", "text analysis", "topic models", "LDA"]
weight: 2
draft: false
output: html_document
---

<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/jquery/jquery.min.js"></script>
<link href="/rmarkdown-libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
<script src="/rmarkdown-libs/datatables-binding/datatables.js"></script>
<link href="/rmarkdown-libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="/rmarkdown-libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="/rmarkdown-libs/dt-core/js/jquery.dataTables.min.js"></script>
<link href="/rmarkdown-libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="/rmarkdown-libs/crosstalk/js/crosstalk.min.js"></script>


<style type="text/css">
/* this chunk of code centers all of the headings */
h1, h2, h3 {
  text-align: center;
}
</style>
<div id="approach" class="section level2">
<h2>Approach</h2>
<p>Once we gathered a corpus of scales from a variety of sources, we wanted to be able to analyze these scales cohesively. Latent Dirichlet Allocation (LDA) is a probabalistic topic modeling process which considers each document as a distribution of topics and each topic as a distribution of words. Using Dirichlet probability distribution, the model draws out the implicit themes across a set of documents. We choose LDA because it is a more commonly used topic modeling process which members of the team had some familiarity with. For this particular implementation of LDA, we used the <a href="http://www.luigicurini.com/uploads/6/7/9/8/67985527/stmvignette.pdf">STM package in R</a>. The goal was to see if the latent topics identified by the model were related to performance or the indicators of performance as outlined in <a href="https://www.researchgate.net/publication/51508445_Conceptual_Frameworks_of_Individual_Work_Performance">Koopmans et al</a>.</p>
</div>
<div id="choosing-our-parameters" class="section level2">
<h2>Choosing our Parameters</h2>
<p>At this particular stage in the process, LDA may not be the most appropriate method, as it is works best with larger datasets. We are seeking, at this point, to demonstrate that with a larger corpus of job performance scales, this approach could lead valuable results. Because of our limited dataset, we took certain steps to make our corpus as large as possible. Typically, a standard set of stop words are removed from the corpus. Stop words are extremely common words which help to form the structure of sentences, but do not inform the content. We did not want to remove this standard stopwords list for two reasons. First, to maximize our corpus, and second, to include pronouns such as &quot;I&quot; and &quot;You&quot; which provide some information about the wording of scale items and the level of analysis. To construct our own stopwords list, we looked at the most frequent words in the dataset.</p>
<p><div id="htmlwidget-1" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47"],["the","to","i","of","my","work","you","job","in","your","how","do","a","and","is","with","or","on","are","have","this","for","that","feel","about","me","at","does","am","as","people","what","organization","well","be","think","when","can","from","not","would","get","supervisor","time","following","much","things"],[1260,1029,963,900,877,815,681,598,558,514,429,413,400,381,376,330,306,302,277,272,262,259,255,221,206,187,179,163,161,151,151,151,150,141,138,123,120,119,113,111,109,108,108,105,102,102,102]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>word<\/th>\n      <th>n<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":2},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script><img src="/findings/performance_lda_files/figure-html/word-frequencies-2.png" width="672" /></p>
<p>Along with the most frequent words, we also included as stop words words refering to specific jobs within the performance scales. These included a number of scales which referred to nursing and hospitals along with locations, such as Britain and Newfoundland.</p>
</div>
<div id="number-of-topics" class="section level2">
<h2>Number of Topics</h2>
<p>In LDA topic modeling, the user inputs the number of topics for the model. Choosing this number of topics can be difficult, but the STM package provides some guidance and different tools for the evaluation of potential numbers of topics. Typically, the main measure of the strength of the model and the desired number of topics is semantic coherence. In addition to semantic coherence, STM also recommends using held-out likelihood, residuals, and exclusivity as measures of model fit. These graphs show the values of those variables on 10 LDA models, with the number of topics ranging from 6 to 24.</p>
<p><img src="/findings/performance_lda_files/figure-html/kresult-plot-1.png" width="672" /><img src="/findings/performance_lda_files/figure-html/kresult-plot-2.png" width="672" /></p>
<p>An ideal model would have a high held-out likelihood, a low level of residuals, and a combination of high semantic coherence and exclusivity. STM recommends looking at semantic coherence by exlcusivtity because semantic coherenence can be &quot;artifically&quot; achieved with very common words amoung topics.</p>
</div>
<div id="fittting-the-model" class="section level2">
<h2>Fittting the model</h2>
<p>When using STM with LDA modeling, STM recommends running a model selection process. With the number of topics chosen from the model above, multiple attempts at LDA to find the most ideal model based again on semantic coherence and exclusivity of topics.</p>
<p><img src="/findings/performance_lda_files/figure-html/model-select-output-1.png" width="672" /></p>
<p>With the best model, the one with the highest exclusivity and semantic coherence, closest to the top right corner of the graph, chosen, we can then visualize the topic model results.</p>
</div>
<div id="visualizations-of-the-topic-model" class="section level2">
<h2>Visualizations of the Topic Model</h2>
<p>The outputs from the topic model reveal some of the latent themes in the job performance scales. We can start to see some themes being pulled out, such as ideal job characteritics, feelings about supervisors, satisfaction, and working in groups.</p>
<p><div id="htmlwidget-2" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-2">{"x":{"filter":"none","data":[["phrases","past","caused","feel","member","felt","workgroup","whole","ask","characteristic","euro","climate"],["think","days","explains","chance","occur","spouse","occupation","itself","try","ideally","quota","considerations"],["describe","did","group","about","environment","area","department","firm","myself","profession","products","ideal"],["words","health","lets","my","simply","were","im","opportunity","project","organization","customer","relation"],["each","rate","members","way","yourself","bothered","behaviors","requires","sure","would","ethics","superior"],["following","many","his","make","performing","disability","plan","piece","team","really","ethnical","lack"],["well","miss","her","job","opportunities","responsibility","months","jobs","challenging","leaving","families","ability"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>V1<\/th>\n      <th>V2<\/th>\n      <th>V3<\/th>\n      <th>V4<\/th>\n      <th>V5<\/th>\n      <th>V6<\/th>\n      <th>V7<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script><img src="/findings/performance_lda_files/figure-html/model-output-2.png" width="672" /><img src="/findings/performance_lda_files/figure-html/model-output-3.png" width="672" /><img src="/findings/performance_lda_files/figure-html/model-output-4.png" width="672" /><img src="/findings/performance_lda_files/figure-html/model-output-5.png" width="672" /><img src="/findings/performance_lda_files/figure-html/model-output-6.png" width="672" /><img src="/findings/performance_lda_files/figure-html/model-output-7.png" width="672" /><img src="/findings/performance_lda_files/figure-html/model-output-8.png" width="672" /></p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>Although we do not necessarily see clearly all aspects of the job performace frameworks in every topic, some have started to emerge from this technique. With the further compliation of scales and the growth of the dataset, these techiques would likely yield valuable results in the development of a framework of job performance specifically for the Army.</p>
</div>
