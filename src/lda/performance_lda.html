---
title: "Job Performance Scales: LDA Topic Modeling"
description: "This page demonstrates an LDA Topic Modeling approach to our corpus."
tags: ["R", "text analysis", "topic models", "LDA"]
author: "Liz Miller"
date: "7/21/2020"
output: html_document
---



<div id="data-setup" class="section level2">
<h2>Data Setup</h2>
<p>This is the part where I am importing the data. I choose to combine the STEM and the ITEM so both parts of the scale were included. I also collapse the scales by citation, so each document is a full scale rather than a part of a scale.</p>
<pre class="r"><code>job_perf &lt;- read_excel(&quot;/project/class/bii_sdad_dspg/uva/dod_ari2/DSPG-ARI2 -- Performance Item and Scale Database.xlsx&quot;,  sheet = &quot;Performance Items&quot;)

job_perf &lt;- job_perf %&gt;%
  filter(CITATION != &quot;Borman, W. C., Motowidlo, S. J., Rose, S. R., &amp; Hanser, L. M. (1987). Development of a Model of Soldier Effectiveness: Retranslation Materials and Results. HUMAN RESOURCES RESEARCH ORGANIZATION ALEXANDRIA VA.&quot;)

#Grouping by scale source/citation if we want to consider texts as documents
cols &lt;- c(&#39;STEM&#39;, &#39;ITEM&#39;)
job_perf$scale &lt;- apply( job_perf[ , cols ], 1 , paste, collapse = &quot; &quot;)
job_perf$scale &lt;- gsub(&quot;NA&quot;,&#39; &#39;, job_perf$scale)

job_perf &lt;- job_perf %&gt;%
  group_by(CITATION, SCALE_NAME, SOURCE_DOMAIN, YEAR) %&gt;%
  summarise(scale = paste0(scale, collapse = &quot; &quot;))</code></pre>
<pre><code>## `summarise()` regrouping output by &#39;CITATION&#39;, &#39;SCALE_NAME&#39;, &#39;SOURCE_DOMAIN&#39; (override with `.groups` argument)</code></pre>
</div>
<div id="pre-processing" class="section level2">
<h2>Pre-Processing</h2>
<p>These are the cleaning and pre-processing steps. To start with, I left all words in and only removed numbers and punctuation. Stemming also occurs in this step.</p>
<pre class="r"><code>stop.words &lt;- c(&quot;the&quot;, &quot;a&quot;, &quot;an&quot;, &quot;for&quot;, &quot;and&quot;, &quot;nor&quot;, &quot;but&quot;, &quot;or&quot;, &quot;yet&quot;, &quot;so&quot;, &quot;either&quot;, &quot;or&quot;, &quot;both&quot;, &quot;if&quot;, &quot;as&quot;, &quot;than&quot;, &quot;how&quot;, &quot;that&quot;, &quot;whatever&quot;, &quot;which&quot;, &quot;whichever&quot;, &quot;where&quot;, &quot;wherever&quot;, &quot;it&quot;, &quot;at&quot;, &quot;to&quot;, &quot;like&quot;, &quot;are&quot;, &quot;from&quot;, &quot;do&quot;, &quot;of&quot;, &quot;andor&quot;, &quot;goran&quot;, &quot;nurse&quot;, &quot;nursing&quot;, &quot;nurses&quot;, &quot;patient&quot;, &quot;hospital&quot;)

processed.scale &lt;- textProcessor(documents = job_perf$scale, metadata = job_perf, lowercase = TRUE, 
                                 removestopwords = FALSE, removenumbers = TRUE,
                                 removepunctuation = TRUE, ucp = TRUE, stem = FALSE,
                                 wordLengths = c(0, Inf), sparselevel = 1, language = &quot;en&quot;,
                                 verbose = FALSE, onlycharacter = FALSE, striphtml = FALSE,
                                 customstopwords = stop.words, custompunctuation = NULL, v1 = FALSE)

out.scale &lt;- prepDocuments(processed.scale$documents, vocab = processed.scale$vocab,
                           meta = processed.scale$meta, lower.thresh = 0, upper.thresh = Inf, 
                           verbose = FALSE)</code></pre>
</div>
<div id="choosing-number-of-topics" class="section level2">
<h2>Choosing number of topics</h2>
<p>Choosing the number of topics is not really a science, but the stm package has attempted to provide some guidance and tools for evaluation.</p>
<p>I think you want a high held-out likelihood, low residuals, and a combination of high semantic coherence and high exclusivity. The authors of the stm paper use exclusiviety in conjunction with semantic coherence:</p>
<p>&quot;Having high semantic coherence is relatively easy, though, if you only have a few topics dominated by very common words, so you want to look at both semantic coherence and exclusivity of words to topics. Itâ€™s a tradeoff.&quot;&quot;</p>
<pre class="r"><code>kresult &lt;- searchK(out.scale$documents, out.scale$vocab, init.type = &quot;LDA&quot;, 
                   K = c(6, 8, 10, 12, 14, 16, 18, 20, 22), seed = 2020, data = out.scale$meta, verbose = FALSE) 
plot(kresult)</code></pre>
<p><img src="/findings/performance_lda_files/figure-html/kresult-1.png" width="672" /></p>
<pre class="r"><code>kresult$results %&gt;%
  select(K, exclus, semcoh) %&gt;%
  filter(K %in% c(6, 8, 10, 12, 14, 16, 18, 20, 22)) %&gt;%
  unnest(cols = c(K, exclus, semcoh)) %&gt;%
  mutate(K = as.factor(K)) %&gt;%
  ggplot(aes(semcoh, exclus, colour = K)) +
  geom_point(size = .01, alpha = 0.7) +
  geom_text(aes(label = K)) +
  labs(x = &quot;Semantic coherence&quot;,
       y = &quot;Exclusivity&quot;,
       title = &quot;Comparing exclusivity and semantic coherence&quot;)</code></pre>
<p><img src="/findings/performance_lda_files/figure-html/kresult-2.png" width="672" /> I chose 7 topics based upon the run and data I had on 7/22.</p>
</div>
<div id="fittting-the-model" class="section level2">
<h2>Fittting the model</h2>
<p>This step is important because I am using the LDA initialization. In the paper, the authors suggest using a Spectral initialization instead of LDA. Spectral is based on LDA but different in the way it starts the model. The authors believe the spectral model is more effective, but I choose LDA because it is what other DSPG projects have used.</p>
<p>This step runs multiple potential models and &quot;recommends&quot; one based on exclusivity and semantic coherence.</p>
<pre class="r"><code>modelSelect &lt;- selectModel(out.scale$documents, out.scale$vocab, K = 14,
                          max.em.its = 200, data = out.scale$meta, runs = 20, init.type = &quot;LDA&quot;, 
                          seed = 2020, verbose = FALSE)</code></pre>
<pre><code>## Casting net 
## 1 models in net 
## 2 models in net 
## 3 models in net 
## 4 models in net 
## 5 models in net 
## 6 models in net 
## 7 models in net 
## 8 models in net 
## 9 models in net 
## 10 models in net 
## 11 models in net 
## 12 models in net 
## 13 models in net 
## 14 models in net 
## 15 models in net 
## 16 models in net 
## 17 models in net 
## 18 models in net 
## 19 models in net 
## 20 models in net 
## Running select models 
## 1 select model run 
## 2 select model run 
## 3 select model run 
## 4 select model run</code></pre>
<pre class="r"><code>plotModels(modelSelect)</code></pre>
<p><img src="/findings/performance_lda_files/figure-html/model_select-1.png" width="672" /></p>
<pre class="r"><code>perf_model &lt;- modelSelect$runout[[2]] </code></pre>
</div>
<div id="visualizations-of-the-topic-model" class="section level2">
<h2>Visualizations of the Topic Model</h2>
<p>Word Descriptions from the STM paper: &quot;The function by default prints several different types of word profiles, including highest probability words and FREX words. FREX weights words by their overall frequency and how exclusive they are to the topic...Lift weights words by dividing by their frequency in other topics, therefore giving higher weight to words that appear less frequently in other topics...Similar to lift, score divides the log frequency of the word in the topic by the log frequency of the word in other topics.&quot;</p>
<pre class="r"><code>labelTopics(perf_model)</code></pre>
<pre><code>## Topic 1 Top Words:
##       Highest Prob: i, my, work, have, in, with, am 
##       FREX: department, newfoundland, plan, senior, labrador, role, government 
##       Lift: labrador, newfoundland, abilitis, accepting, acheivements, achieveemnt, agencies 
##       Score: department, newfoundland, plan, i, labrador, senior, role 
## Topic 2 Top Words:
##       Highest Prob: you, your, work, in, with, have, is 
##       FREX: yourself, caused, you, did, were, felt, during 
##       Lift: accident, atmosphere, basis, come, disability, failure, miss 
##       Score: you, your, yourself, caused, past, felt, did 
## Topic 3 Top Words:
##       Highest Prob: my, job, i, on, is, this, feel 
##       FREX: chance, feel, this, on, about, is, job 
##       Lift: steady, administered, againt, ahread, avoided, backs, cheating 
##       Score: this, chance, feel, about, i, my, job 
## Topic 4 Top Words:
##       Highest Prob: your, work, you, in, with, group, is 
##       FREX: occupation, ideal, group, considerations, members, your, relation 
##       Lift: assign, britain, heshe, indicate, occupation, points, total 
##       Score: your, you, occupation, group, ideal, following, considerations 
## Topic 5 Top Words:
##       Highest Prob: my, supervisor, i, me, can, in, company 
##       FREX: euro, quota, ask, company, supervisor, he, behavior 
##       Lift: ethnical, grade, salespeople, sell, additional, approves, asks 
##       Score: supervisor, euro, quota, behavior, selling, customer, ask 
## Topic 6 Top Words:
##       Highest Prob: job, you, work, your, in, i, this 
##       FREX: characteristic, ideally, extent, opportunity, would, much, whether 
##       Lift: finished, independent, acutal, amoung, arranged, attempts, attract 
##       Score: you, ideally, characteristic, your, extent, this, would 
## Topic 7 Top Words:
##       Highest Prob: in, work, patients, procedures, with, unit, assignments 
##       FREX: fails, patients, families, improvements, timely, assignments, unit 
##       Lift: accountability, adapts, developmental, families, helping, improvements, regulations 
##       Score: patients, unit, program, manner, fails, needs, families 
## Topic 8 Top Words:
##       Highest Prob: i, work, my, in, have, im, can 
##       FREX: im, colleagues, exposed, must, carried, hours, pace 
##       Lift: carried, adversely, aggression, agreeable, anger, anything, arent 
##       Score: im, i, exposed, colleagues, carried, must, because 
## Topic 9 Top Words:
##       Highest Prob: i, my, organization, would, me, feel, not 
##       FREX: profession, organization, really, would, leave, who, organizations 
##       Lift: absence, attached, especially, normally, oriented, absense, absorbed 
##       Score: organization, would, profession, i, really, now, who 
## Topic 10 Top Words:
##       Highest Prob: i, my, in, job, work, you, have 
##       FREX: workgroup, months, behaviors, past, been, subjected, requires 
##       Lift: acknowledgement, afffect, asleep, assisted, attend, automated, automation 
##       Score: workgroup, behaviors, you, months, subjected, past, following 
## Topic 11 Top Words:
##       Highest Prob: think, does, well, each, work, describe, following 
##       FREX: describe, think, phrases, each, words, following, well 
##       Lift: agreements, annoying, bairly, boring, co-operation, crises, date 
##       Score: phrases, words, describe, following, each, majority, whom 
## Topic 12 Top Words:
##       Highest Prob: work, i, in, things, job, get, about 
##       FREX: things, take, more, get, was, willing, life 
##       Lift: worth, show, unemployment, willing, involved, having, only 
##       Score: life, things, i, was, friends, more, part 
## Topic 13 Top Words:
##       Highest Prob: opportunities, your, work, you, my, occur, does 
##       FREX: occur, opportunities, member, simply, environment, performing, duties 
##       Lift: interact, member, recognized, asssociated, class, condtions, dead-end 
##       Score: occur, member, simply, you, your, performing, environment 
## Topic 14 Top Words:
##       Highest Prob: i, my, job, am, work, what, in 
##       FREX: over, am, health, we, objectives, interesting, problem 
##       Lift: accounts, adjusting, allowed, annual, aspect, contributing, definitely 
##       Score: i, am, my, modify, accomplish, health, we</code></pre>
<pre class="r"><code>topicQuality(perf_model, out.scale$documents)</code></pre>
<pre><code>##  [1] -37.49307 -32.28410 -28.09203 -29.85868 -32.04608 -20.70171 -55.53388
##  [8] -37.37033 -21.88735 -48.51886 -84.33614 -23.80541 -54.90087 -11.02480
##  [1] 9.247178 8.413603 9.777831 8.788235 9.259674 9.044334 8.860571 9.221777
##  [9] 8.597378 9.004477 9.819869 9.802290 9.617331 9.487670</code></pre>
<p><img src="/findings/performance_lda_files/figure-html/model_output-1.png" width="672" /></p>
<pre class="r"><code>mod.out.corr &lt;- topicCorr(perf_model)
plot(mod.out.corr)</code></pre>
<p><img src="/findings/performance_lda_files/figure-html/model_output-2.png" width="672" /></p>
<pre class="r"><code>plot(perf_model, type = &quot;summary&quot;, xlim = c(0, 1), labeltype = &quot;frex&quot;, n = 5)</code></pre>
<p><img src="/findings/performance_lda_files/figure-html/model_output-3.png" width="672" /></p>
<pre class="r"><code>findThoughts(perf_model, texts = job_perf$SCALE_NAME)</code></pre>
<pre><code>## 
##  Topic 1: 
##       Work Environment Survey
##      The Team KSA Test (15-TKT)
##      NA 
##  Topic 2: 
##       NA
##      WHO Health and Performance Questionnaire
##      NA 
##  Topic 3: 
##       Minnesota Satisfaction Questionnaire
##      Job Satisfaction Survey
##      NA 
##  Topic 4: 
##       Psychological and Social Factors at Work
##      Empowering Leadership Questionnaire
##      Work Organization Assessment Questionnaire 
##  Topic 5: 
##       NA
##      NA
##      NA 
##  Topic 6: 
##       Job Characteristics Inventory
##      work attitude measures of trust
##      NA 
##  Topic 7: 
##       NA
##      HHS PMAP Handbook
##      NA 
##  Topic 8: 
##       Working Conditions and Control Questionnaire
##      NA
##      Standford Presenteeism Scale 
##  Topic 9: 
##       Commitment to Organizations and Occupations:
##      NA
##      Organizational Commitment Questionnaire 
##  Topic 10: 
##       People at Work Survey
##      Stress &amp; Satisfaction Offset Score
##      Multimethod Job Design Questionnaire 
##  Topic 11: 
##       NA
##      NA
##      NA 
##  Topic 12: 
##       NA
##      NA
##      Role-Based Performance Scale 
##  Topic 13: 
##       NA
##      Worker Opinion Survey
##      NA 
##  Topic 14: 
##       NA
##      NA
##      Job Ambiguity</code></pre>
<pre class="r"><code>wordclouds &lt;- function(model, topic) {
  cloud(model, topic = topic)
}

topicnum &lt;- 1:14
for (i in topicnum) {
  wordclouds(perf_model, i)
}</code></pre>
<p><img src="/findings/performance_lda_files/figure-html/model_output-4.png" width="672" /><img src="/findings/performance_lda_files/figure-html/model_output-5.png" width="672" /><img src="/findings/performance_lda_files/figure-html/model_output-6.png" width="672" /><img src="/findings/performance_lda_files/figure-html/model_output-7.png" width="672" /><img src="/findings/performance_lda_files/figure-html/model_output-8.png" width="672" /><img src="/findings/performance_lda_files/figure-html/model_output-9.png" width="672" /><img src="/findings/performance_lda_files/figure-html/model_output-10.png" width="672" /><img src="/findings/performance_lda_files/figure-html/model_output-11.png" width="672" /><img src="/findings/performance_lda_files/figure-html/model_output-12.png" width="672" /><img src="/findings/performance_lda_files/figure-html/model_output-13.png" width="672" /></p>
<pre><code>## Warning in wordcloud::wordcloud(words = vocab, freq = vec, max.words =
## max.words, : phrases could not be fit on page. It will not be plotted.</code></pre>
<pre><code>## Warning in wordcloud::wordcloud(words = vocab, freq = vec, max.words =
## max.words, : following could not be fit on page. It will not be plotted.</code></pre>
<p><img src="/findings/performance_lda_files/figure-html/model_output-14.png" width="672" /><img src="/findings/performance_lda_files/figure-html/model_output-15.png" width="672" /></p>
<pre><code>## Warning in wordcloud::wordcloud(words = vocab, freq = vec, max.words =
## max.words, : work could not be fit on page. It will not be plotted.</code></pre>
<p><img src="/findings/performance_lda_files/figure-html/model_output-16.png" width="672" /><img src="/findings/performance_lda_files/figure-html/model_output-17.png" width="672" /></p>
</div>
<div id="the-end" class="section level2">
<h2>The End</h2>
</div>
