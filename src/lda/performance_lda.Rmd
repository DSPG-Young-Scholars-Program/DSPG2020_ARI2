---
title: "Job Performance Scales: LDA Topic Modeling"
description: "This page demonstrates an LDA Topic Modeling approach to our corpus."
tags: ["R", "text analysis", "topic models", "LDA"]
author: "Liz Miller"
date: "7/21/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyverse)
library(tidytext)
library(stm)
library(quanteda)
library(ggplot2)
library(readr)
library(readxl)
set.seed(2020)
```

## Data Setup

This is the part where I am importing the data. I choose to combine the STEM and the ITEM so both parts of the scale were included. I also collapse the scales by citation, so each document is a full scale rather than a part of a scale. 

```{r read_data}

job_perf <- read_excel("/project/class/bii_sdad_dspg/uva/dod_ari2/DSPG-ARI2 -- Performance Item and Scale Database.xlsx",  sheet = "Performance Items")

job_perf <- job_perf %>%
  filter(CITATION != "Borman, W. C., Motowidlo, S. J., Rose, S. R., & Hanser, L. M. (1987). Development of a Model of Soldier Effectiveness: Retranslation Materials and Results. HUMAN RESOURCES RESEARCH ORGANIZATION ALEXANDRIA VA.")

#Grouping by scale source/citation if we want to consider texts as documents
cols <- c('STEM', 'ITEM')
job_perf$scale <- apply( job_perf[ , cols ], 1 , paste, collapse = " ")
job_perf$scale <- gsub("NA",' ', job_perf$scale)

job_perf <- job_perf %>%
  group_by(CITATION, SCALE_NAME, SOURCE_DOMAIN, YEAR) %>%
  summarise(scale = paste0(scale, collapse = " "))
```

## Pre-Processing
These are the cleaning and pre-processing steps. To start with, I left all words in and only removed numbers and punctuation. Stemming also occurs in this step. 

```{r pre-processing}

stop.words <- c("the", "a", "an", "for", "and", "nor", "but", "or", "yet", "so", "either", "or", "both", "if", "as", "than", "how", "that", "whatever", "which", "whichever", "where", "wherever", "it", "at", "to", "like", "are", "from", "do", "of", "andor", "goran", "nurse", "nursing", "nurses", "patient", "hospital")

processed.scale <- textProcessor(documents = job_perf$scale, metadata = job_perf, lowercase = TRUE, 
                                 removestopwords = FALSE, removenumbers = TRUE,
                                 removepunctuation = TRUE, ucp = TRUE, stem = FALSE,
                                 wordLengths = c(0, Inf), sparselevel = 1, language = "en",
                                 verbose = FALSE, onlycharacter = FALSE, striphtml = FALSE,
                                 customstopwords = stop.words, custompunctuation = NULL, v1 = FALSE)

out.scale <- prepDocuments(processed.scale$documents, vocab = processed.scale$vocab,
                           meta = processed.scale$meta, lower.thresh = 0, upper.thresh = Inf, 
                           verbose = FALSE)

```

##Choosing number of topics

Choosing the number of topics is not really a science, but the stm package has attempted to provide some guidance and tools for evaluation. 

I think you want a high held-out likelihood, low residuals, and a combination of high semantic coherence and high exclusivity. The authors of the stm paper use exclusiviety in conjunction with semantic coherence: 

"Having high semantic coherence is relatively easy, though, if you only have a few topics dominated by very common words, so you want to look at both semantic coherence and exclusivity of words to topics. Itâ€™s a tradeoff."" 

``` {r kresult}

kresult <- searchK(out.scale$documents, out.scale$vocab, init.type = "LDA", 
                   K = c(6, 8, 10, 12, 14, 16, 18, 20, 22), seed = 2020, data = out.scale$meta, verbose = FALSE) 
plot(kresult)

kresult$results %>%
  select(K, exclus, semcoh) %>%
  filter(K %in% c(6, 8, 10, 12, 14, 16, 18, 20, 22)) %>%
  unnest(cols = c(K, exclus, semcoh)) %>%
  mutate(K = as.factor(K)) %>%
  ggplot(aes(semcoh, exclus, colour = K)) +
  geom_point(size = .01, alpha = 0.7) +
  geom_text(aes(label = K)) +
  labs(x = "Semantic coherence",
       y = "Exclusivity",
       title = "Comparing exclusivity and semantic coherence")

```
I chose 7 topics based upon the run and data I had on 7/22. 

##Fittting the model

This step is important because I am using the LDA initialization. In the paper, the authors suggest using a Spectral initialization instead of LDA. Spectral is based on LDA but different in the way it starts the model. The authors believe the spectral model is more effective, but I choose LDA because it is what other DSPG projects have used. 

This step runs multiple potential models and "recommends" one based on exclusivity and semantic coherence. 

``` {r model_select}

modelSelect <- selectModel(out.scale$documents, out.scale$vocab, K = 14,
                          max.em.its = 200, data = out.scale$meta, runs = 20, init.type = "LDA", 
                          seed = 2020, verbose = FALSE)
plotModels(modelSelect)

perf_model <- modelSelect$runout[[2]] 

```



##Visualizations of the Topic Model

Word Descriptions from the STM paper: 
"The function by default prints several different types of word profiles, including highest probability words and FREX words. FREX weights words by their overall frequency and how exclusive they are to the
topic...Lift weights words by dividing by their frequency in other topics, therefore giving higher weight to words that appear less frequently in other topics...Similar to lift, score divides the log frequency of the word in the topic by the log frequency of the word in other topics."


``` {r model_output}
labelTopics(perf_model)

topicQuality(perf_model, out.scale$documents)

mod.out.corr <- topicCorr(perf_model)
plot(mod.out.corr)

plot(perf_model, type = "summary", xlim = c(0, 1), labeltype = "frex", n = 5)

findThoughts(perf_model, texts = job_perf$SCALE_NAME)

wordclouds <- function(model, topic) {
  cloud(model, topic = topic)
}

topicnum <- 1:14
for (i in topicnum) {
  wordclouds(perf_model, i)
}
```

##The End
