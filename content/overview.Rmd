---
title: "Open Source Software 2020 Summer Project"
output: html_document
---

```{css, echo=FALSE}
/* this chunnk of code centers all of the headings */
h1, h2, h3 {
  text-align: center;
}
```

### Project Overview 

According to the [Data Manpower Data Center (DMDC)](https://www.dmdc.osd.mil/appj/dwp/dwp_reports.jsp) the United States Army approximately 475 thousand active duty, and approximately 190 thousand reserve, service members.  As part of its standard personnel management practices it collects a number of measures and/or surveys from and about these service members.  Given the scope, depth, duration of this data source, the U.S. Army now seeks to determine if this information can be leveraged to predict behavioral performance outcomes for service members and if it might otherwise be useful to augment its workforce management practices.

### Project Goals and Approach

In order to provide our stakeholder, the [Army Research Institute](https://ari.altess.army.mil/), with pertinant insight regarding the fesability of predicting behavioral performance using existing data sources we must first identify existing authoratative behavioral performance items from academic, industry, government and military sources.  In essence, the collection of this corpus of documents will serve as the basis from which a consensus regarding themes and topics relating to behavioral performance will be extracted.  Thus our first goal was:

>To identify documents from academic, industry, government, and military >sources which contain assessment scale items measuring various manifestations >and/or notions of performance.

Having achived this, we now needed to extract the individual assesment items (i.e. "questions") from each of these documents so that the terms *explicitly* used in the assesment items (and, presumably, the latent variables they correspond to) could be subjected to analysis.  Furethermore, in order to maximally facilitate secondary analysis, additional characteristics were manually ascribed to the assesment items, based on a number of criteria.  Thus, our second goal was: 

>For each of the documents identified in the previous step: extract each >assessment item present in the document and characterize it based on (1) >provenance; (2) general theme, content, and target; (3) Koopmans et al. >framework features and characteristics; and (4) quality/quantity of assessment >scale items.

With the assesment items extracted, we then needed to determine what common topics and themes they shared.  Thus our third goal was:

>Perform a text analysis on the extracted items, and attempt to characterize >the topic groups and their relations

With the litterature's themes now avaiable to us, we would next be able to map these back on to the data collection tools used by the military.  Thus our fourth and final goal was:

>Time permitting, compare these groups to assessment items and tools used by >the military in order to assess overlap and determine suitability of those >items for potential application to military personnel. 

### Our Approach 

Example text: Over the course of the 2020 Data Science for the Public Good Program, we worked to classify OSS contributors into these sectors, count which institutions users are affiliated within each sector, and researched how users collaborate within and across these various sectors. In our project, we implemented various methods, including web scraping (to collect the data), natural language processing (to match and recode user affilations), and social network analysis (to examine collaboration tendencies).